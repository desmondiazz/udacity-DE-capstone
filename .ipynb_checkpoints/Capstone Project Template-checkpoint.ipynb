{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col , to_date , monotonically_increasing_id , when,trim , explode , array , struct,expr,concat_ws\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format , dayofweek,lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType , LongType , DoubleType , TimestampType\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=com.amazonaws:aws-java-sdk-bundle:1.11.271,org.apache.hadoop:hadoop-aws:3.1.2 pyspark-shell\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    ".config(\"spark.jars.packages\",\"com.amazonaws:aws-java-sdk-pom:1.10.34\") \\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Scope of this project is to understand what crops are sown in India , and what was the avg weather during that time period. This will help us in understanding how agriculture pattern has evolved in India based on the weather changes over the years.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "I will be using using two datasets for this project\n",
    "##### 1: District-wise, season-wise crop production statistics from 1997 (https://data.gov.in/resources/district-wise-season-wise-crop-production-statistics-1997)\n",
    "* This data includes crop production in India from 1997-2015\n",
    "* This Includes what was the crops produced in a particular state. And its production quantity(Tonnes) and area(Hectare) \n",
    "\n",
    "##### 2: World Temperature Data (https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)\n",
    "* This data includes temperature data from 1743 - 2013\n",
    "* The world temperature dataset has multiple datasets under it , for the purpose of this project i will be using GlobalLandTemperaturesByState.csv dataset . As this perticular dataset is consist weather for a state in a perticular country on a perticular day.\n",
    "\n",
    "#### Information about the datasets\n",
    "* Both the datasets are of type csv.\n",
    "* Combined together both the datasets consists of over 891766 rows.\n",
    "* World tempertature data will be filtered based on multiple conditions such as country as India and min and maximum years as per the crop production data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "world_crop_produce_data = spark.read.option('header',True).csv('./data/worldwide-crop-production/original/*')\n",
    "world_temp_data = spark.read.option('header',True).csv('./data/world_temperature_data/GlobalLandTemperaturesByCountry.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusable udf's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trim spaces of strings\n",
    "trim_str_udf = udf(lambda x : str(x.strip()))\n",
    "\n",
    "#Parse string to int\n",
    "str_to_int_udf = udf(lambda x: int(x))\n",
    "\n",
    "#Extract year(int) from datetime string y-m-d\n",
    "extract_year_udf = udf(lambda x: datetime.strptime(x, '%Y-%m-%d').year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose world-crop-produce year columns to rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------+--------------------+------------+--------------+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+--------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+\n",
      "|Area Code|       Area|Item Code|                Item|Element Code|       Element|  Unit|Y1961|Y1961F|Y1962|Y1962F|Y1963|Y1963F|Y1964|Y1964F|Y1965|Y1965F|Y1966|Y1966F|Y1967|Y1967F|Y1968|Y1968F|Y1969|Y1969F|Y1970|Y1970F|Y1971|Y1971F|Y1972|Y1972F|Y1973|Y1973F|Y1974|Y1974F|   Y1975|Y1975F|       Y1976|Y1976F|       Y1977|Y1977F|       Y1978|Y1978F|       Y1979|Y1979F|       Y1980|Y1980F|       Y1981|Y1981F|       Y1982|Y1982F|       Y1983|Y1983F|       Y1984|Y1984F|       Y1985|Y1985F|       Y1986|Y1986F|       Y1987|Y1987F|       Y1988|Y1988F|       Y1989|Y1989F|       Y1990|Y1990F|       Y1991|Y1991F|       Y1992|Y1992F|       Y1993|Y1993F|       Y1994|Y1994F|       Y1995|Y1995F|       Y1996|Y1996F|       Y1997|Y1997F|       Y1998|Y1998F|       Y1999|Y1999F|       Y2000|Y2000F|       Y2001|Y2001F|       Y2002|Y2002F|       Y2003|Y2003F|       Y2004|Y2004F|       Y2005|Y2005F|       Y2006|Y2006F|       Y2007|Y2007F|       Y2008|Y2008F|       Y2009|Y2009F|       Y2010|Y2010F|       Y2011|Y2011F|       Y2012|Y2012F|       Y2013|Y2013F|       Y2014|Y2014F|\n",
      "+---------+-----------+---------+--------------------+------------+--------------+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+--------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+\n",
      "|        2|Afghanistan|      221| Almonds, with shell|        5312|Area harvested|    ha| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null|0.000000|     F| 5900.000000|     F| 6000.000000|     F| 6000.000000|     F| 6000.000000|     F| 5800.000000|     F| 5800.000000|     F| 5800.000000|     F| 5700.000000|     F| 5700.000000|     F| 5600.000000|     F| 5500.000000|     F| 5500.000000|     F| 5400.000000|     F| 5400.000000|     F| 6010.000000|    Im| 5500.000000|  null| 5500.000000|  null| 5500.000000|  null| 5500.000000|  null| 5500.000000|  null| 5500.000000|  null| 5500.000000|  null| 5500.000000|  null| 5500.000000|  null| 7000.000000|     F| 9000.000000|     F| 5500.000000|  null| 5700.000000|  null|12000.000000|     *|10635.000000|    Im|12000.000000|  null|12000.000000|  null|12000.000000|  null|11029.000000|  null|11210.000000|  null|13469.000000|  null|13490.000000|  null|14114.000000|  null|13703.000000|  null|\n",
      "|        2|Afghanistan|      221| Almonds, with shell|        5419|         Yield| hg/ha| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null|    null|  null|16610.000000|    Fc|15000.000000|    Fc|20000.000000|    Fc|17500.000000|    Fc|17069.000000|    Fc|13793.000000|    Fc|18966.000000|    Fc|17018.000000|    Fc|18421.000000|    Fc|16071.000000|    Fc|18182.000000|    Fc|16364.000000|    Fc|16667.000000|    Fc|16296.000000|    Fc|15807.000000|    Fc|16364.000000|    Fc|18000.000000|    Fc|16364.000000|    Fc|16364.000000|    Fc|16364.000000|    Fc|16364.000000|    Fc|16364.000000|    Fc|16364.000000|    Fc|20000.000000|    Fc|17143.000000|    Fc|16667.000000|    Fc|20514.000000|    Fc|24561.000000|    Fc|12250.000000|    Fc|14696.000000|    Fc|16667.000000|    Fc|26234.000000|    Fc|35000.000000|    Fc|39154.000000|    Fc|49955.000000|    Fc|45000.000000|    Fc|45960.000000|    Fc|29910.000000|    Fc|19996.000000|    Fc|\n",
      "|        2|Afghanistan|      221| Almonds, with shell|        5510|    Production|tonnes| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null|0.000000|     F| 9800.000000|     F| 9000.000000|     F|12000.000000|     F|10500.000000|     F| 9900.000000|     F| 8000.000000|     F|11000.000000|     F| 9700.000000|     F|10500.000000|     F| 9000.000000|     F|10000.000000|     F| 9000.000000|     F| 9000.000000|     F| 8800.000000|     F| 9500.000000|     F| 9000.000000|  null| 9900.000000|  null| 9000.000000|  null| 9000.000000|  null| 9000.000000|  null| 9000.000000|  null| 9000.000000|  null| 9000.000000|  null|11000.000000|     F|12000.000000|     F|15000.000000|     F|11283.000000|    Im|14000.000000|     *|14700.000000|     *|15630.000000|     *|20000.000000|  null|31481.000000|  null|42000.000000|  null|43183.000000|     *|56000.000000|  null|60611.000000|  null|62000.000000|  null|42215.000000|  null|27400.000000|  null|\n",
      "|        2|Afghanistan|      711|Anise, badian, fe...|        5312|Area harvested|    ha| null|     M| null|     M| null|     M| null|     M| null|     M| null|     M| null|     M| null|     M| null|     M| null|     M| null|     M| null|     M| null|     M| null|     M|    null|     M|        null|     M|        null|     M|        null|     M|        null|     M|        null|     M|        null|     M|        null|     M|        null|     M|        null|     M|        null|     M|  700.000000|     F|  700.000000|     F|  300.000000|     F| 1100.000000|     F| 1300.000000|     F| 2128.000000|    Im|  700.000000|     F| 2534.000000|    Im| 3063.000000|    Im| 3000.000000|     F| 7000.000000|     F| 4000.000000|     F|12000.000000|     F|11288.000000|    Im| 4000.000000|     F| 1600.000000|     F| 3300.000000|     F| 6800.000000|     F|15000.000000|     F|16000.000000|     F|18017.000000|    Im|28000.000000|     F|15000.000000|     F|16904.000000|    Im|17000.000000|     F|18505.000000|    Im|18500.000000|     F|18500.000000|     F|20300.000000|    Im|\n",
      "|        2|Afghanistan|      711|Anise, badian, fe...|        5419|         Yield| hg/ha| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null| null|  null|    null|  null|        null|  null|        null|  null|        null|  null|        null|  null|        null|  null|        null|  null|        null|  null|        null|  null|        null|  null|        null|  null| 7143.000000|    Fc| 7143.000000|    Fc| 6667.000000|    Fc| 7273.000000|    Fc| 7692.000000|    Fc| 7016.000000|    Fc| 7143.000000|    Fc| 6818.000000|    Fc| 6734.000000|    Fc| 6667.000000|    Fc| 6000.000000|    Fc| 6250.000000|    Fc| 5917.000000|    Fc| 6201.000000|    Fc| 6250.000000|    Fc| 6250.000000|    Fc| 6061.000000|    Fc| 6029.000000|    Fc| 6000.000000|    Fc| 6250.000000|    Fc| 6208.000000|    Fc| 6071.000000|    Fc| 6000.000000|    Fc| 6140.000000|    Fc| 6000.000000|    Fc| 6304.000000|    Fc| 6757.000000|    Fc| 6757.000000|    Fc| 6513.000000|    Fc|\n",
      "+---------+-----------+---------+--------------------+------------+--------------+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+--------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "world_crop_produce_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------------------+---------+--------------+------------+----+----+-----------+\n",
      "|       Area|Area Code|               Item|Item Code|       Element|Element Code|Unit|year|        qty|\n",
      "+-----------+---------+-------------------+---------+--------------+------------+----+----+-----------+\n",
      "|Afghanistan|        2|Almonds, with shell|      221|Area harvested|        5312|  ha|1961|       null|\n",
      "|Afghanistan|        2|Almonds, with shell|      221|Area harvested|        5312|  ha|1962|       null|\n",
      "|Afghanistan|        2|Almonds, with shell|      221|Area harvested|        5312|  ha|1963|       null|\n",
      "|Afghanistan|        2|Almonds, with shell|      221|Area harvested|        5312|  ha|1964|       null|\n",
      "|Afghanistan|        2|Almonds, with shell|      221|Area harvested|        5312|  ha|1965|       null|\n",
      "|Afghanistan|        2|Almonds, with shell|      221|Area harvested|        5312|  ha|1966|       null|\n",
      "|Afghanistan|        2|Almonds, with shell|      221|Area harvested|        5312|  ha|1967|       null|\n",
      "|Afghanistan|        2|Almonds, with shell|      221|Area harvested|        5312|  ha|1968|       null|\n",
      "|Afghanistan|        2|Almonds, with shell|      221|Area harvested|        5312|  ha|1969|       null|\n",
      "|Afghanistan|        2|Almonds, with shell|      221|Area harvested|        5312|  ha|1970|       null|\n",
      "|Afghanistan|        2|Almonds, with shell|      221|Area harvested|        5312|  ha|1971|       null|\n",
      "|Afghanistan|        2|Almonds, with shell|      221|Area harvested|        5312|  ha|1972|       null|\n",
      "|Afghanistan|        2|Almonds, with shell|      221|Area harvested|        5312|  ha|1973|       null|\n",
      "|Afghanistan|        2|Almonds, with shell|      221|Area harvested|        5312|  ha|1974|       null|\n",
      "|Afghanistan|        2|Almonds, with shell|      221|Area harvested|        5312|  ha|1975|   0.000000|\n",
      "|Afghanistan|        2|Almonds, with shell|      221|Area harvested|        5312|  ha|1976|5900.000000|\n",
      "|Afghanistan|        2|Almonds, with shell|      221|Area harvested|        5312|  ha|1977|6000.000000|\n",
      "|Afghanistan|        2|Almonds, with shell|      221|Area harvested|        5312|  ha|1978|6000.000000|\n",
      "|Afghanistan|        2|Almonds, with shell|      221|Area harvested|        5312|  ha|1979|6000.000000|\n",
      "|Afghanistan|        2|Almonds, with shell|      221|Area harvested|        5312|  ha|1980|5800.000000|\n",
      "+-----------+---------+-------------------+---------+--------------+------------+----+----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "world_crop_produce=world_crop_produce_data.select('Area','Area Code','Item','Item Code','Element','Element Code','Unit','Y1961','Y1962','Y1963','Y1964',\n",
    "'Y1965','Y1966','Y1967','Y1968','Y1969','Y1970','Y1971','Y1972','Y1973','Y1974','Y1975','Y1976','Y1977',\n",
    "'Y1978','Y1979','Y1980','Y1981','Y1982','Y1983','Y1984','Y1985','Y1986','Y1987','Y1988','Y1989','Y1990','Y1991',\n",
    "'Y1992','Y1993','Y1994','Y1995','Y1996','Y1997','Y1998','Y1999','Y2000','Y2001','Y2002','Y2003','Y2004',\n",
    "'Y2005','Y2006','Y2007','Y2008','Y2009','Y2010','Y2011','Y2012','Y2013','Y2014')\n",
    "\n",
    "def to_explode(df, by):\n",
    "\n",
    "    # Filter dtypes and split into column names and type description\n",
    "    cols, dtypes = zip(*((c, t) for (c, t) in df.dtypes if c not in by))\n",
    "    # Spark SQL supports only homogeneous columns\n",
    "    assert len(set(dtypes)) == 1, \"All columns have to be of the same type\"\n",
    "\n",
    "    # Create and explode an array of (column_name, column_value) structs\n",
    "    kvs = explode(array([\n",
    "      struct(lit(c).alias(\"year\"), col(c).alias(\"qty\")) for c in cols\n",
    "    ])).alias(\"kvs\")\n",
    "\n",
    "    return df.select(by + [kvs]).select(by + [\"kvs.year\", \"kvs.qty\"])\n",
    "\n",
    "world_crop_produce = to_explode(world_crop_produce, ['Area','Area Code','Item','Item Code','Element','Element Code','Unit'])\n",
    "world_crop_produce = world_crop_produce.withColumn('year',world_crop_produce.year.substr(2,4))\n",
    "world_crop_produce.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Explore and Assess the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+----------+------------------+-----------------------------+-------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|Country|\n",
      "+----------+------------------+-----------------------------+-------+\n",
      "|1743-11-01|4.3839999999999995|                        2.294|  Åland|\n",
      "|1743-12-01|              null|                         null|  Åland|\n",
      "|1744-01-01|              null|                         null|  Åland|\n",
      "|1744-02-01|              null|                         null|  Åland|\n",
      "|1744-03-01|              null|                         null|  Åland|\n",
      "|1744-04-01|              1.53|                         4.68|  Åland|\n",
      "|1744-05-01| 6.702000000000001|                        1.789|  Åland|\n",
      "|1744-06-01|11.609000000000002|                        1.577|  Åland|\n",
      "|1744-07-01|            15.342|                         1.41|  Åland|\n",
      "|1744-08-01|              null|                         null|  Åland|\n",
      "+----------+------------------+-----------------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "world_temp_data.printSchema()\n",
    "world_temp_data.count()\n",
    "world_temp_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Area: string (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Element: string (nullable = true)\n",
      " |-- Unit: string (nullable = true)\n",
      " |-- year: string (nullable = false)\n",
      " |-- qty: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2255094"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_crop_produce.printSchema()\n",
    "world_crop_produce.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter temperature data based on crop years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract minimum and maximum year in the agriculture data to filter temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1961 2014\n"
     ]
    }
   ],
   "source": [
    "\n",
    "crop_produce_years = world_crop_produce.select('year').distinct()\n",
    "crop_produce_years = crop_produce_years.withColumn('year',str_to_int_udf(crop_produce_years.year))\n",
    "min_crop_produce_year = int(crop_produce_years.agg({'year':'min'}).collect()[0][0])\n",
    "max_crop_produce_year = int(crop_produce_years.agg({'year':'max'}).collect()[0][0])\n",
    "print(min_crop_produce_year,max_crop_produce_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter temperature data based on the minimun and maximun year of agriculture data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153819"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_temperature_data = world_temp_data.withColumn('year',extract_year_udf(world_temp_data.dt))\n",
    "world_temperature_data = world_temperature_data.filter((col('year') >= min_crop_produce_year) & (col('year') <= max_crop_produce_year))\n",
    "world_temperature_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of countries from agriculture data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|             country|country code|\n",
      "+--------------------+------------+\n",
      "|Saint Kitts and N...|         188|\n",
      "|             Romania|         183|\n",
      "|     China, mainland|          41|\n",
      "|            Suriname|         207|\n",
      "|               Yemen|         249|\n",
      "|        Sierra Leone|         197|\n",
      "|            Portugal|         174|\n",
      "|       Faroe Islands|          64|\n",
      "|             R�union|         182|\n",
      "|United Arab Emirates|         225|\n",
      "|              Malawi|         130|\n",
      "|           Mauritius|         137|\n",
      "|            Honduras|          95|\n",
      "|               Tonga|         219|\n",
      "|China, Taiwan Pro...|         214|\n",
      "|           Indonesia|         101|\n",
      "|            Bulgaria|          27|\n",
      "|                Oman|         221|\n",
      "|             Albania|           3|\n",
      "|            Slovenia|         198|\n",
      "+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries = world_crop_produce.select('Area' , 'Area Code').distinct()\n",
    "countries = countries.withColumnRenamed('Area','country').withColumnRenamed('Area Code','country_code')\n",
    "countries.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of items as crops from crops data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                crop|crop_code|\n",
      "+--------------------+---------+\n",
      "|  Flax fibre and tow|      773|\n",
      "|      Safflower seed|      280|\n",
      "| Lettuce and chicory|      372|\n",
      "|            Soybeans|      236|\n",
      "|Grapefruit (inc. ...|      507|\n",
      "|          Sugar beet|      157|\n",
      "|          Chick peas|      191|\n",
      "|              Carobs|      461|\n",
      "|Brazil nuts, with...|      216|\n",
      "|            Cherries|      531|\n",
      "|Cashew nuts, with...|      217|\n",
      "|              Grapes|      560|\n",
      "|            Potatoes|      116|\n",
      "|      Sweet potatoes|      122|\n",
      "|Jute & Jute-like ...|     1751|\n",
      "|     Rubber, natural|      836|\n",
      "|          Poppy seed|      296|\n",
      "|         Berries nes|      558|\n",
      "|             Papayas|      600|\n",
      "|           Nuts, nes|      234|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crops = world_crop_produce.select('Item' , 'Item Code').distinct()\n",
    "crops = crops.withColumnRenamed('Item','crop').withColumnRenamed('Item Code','crop_code')\n",
    "crops.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial crop data counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2255094"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_crop_produce.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter crop produce data where quantity isnot null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_crop_produce = world_crop_produce.where(world_crop_produce.qty.isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1725544"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_crop_produce.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
